{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Control-RL/Function-Approximation --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "class Pi(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Pi, self).__init__()\n",
    "        layers = [\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.onpolicy_reset()\n",
    "        self.train()  # set training mode\n",
    "\n",
    "    def onpolicy_reset(self):\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        pdparam = self.model(x)\n",
    "        return pdparam\n",
    "\n",
    "    def act(self, state):\n",
    "        x = torch.from_numpy(state.astype(np.float32))  # to tensor\n",
    "        pdparam = self.forward(x)  # forward pass\n",
    "        pd = torch.distributions.Categorical(logits=pdparam)  # probability distribution\n",
    "        action = pd.sample()  # pi(a|s) in action via pd\n",
    "        log_prob = pd.log_prob(action)  # log prob of pi(a|s)\n",
    "        self.log_probs.append(log_prob)  # store for training\n",
    "        return action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner gradient loop of basic policy-gradient algorithm (Reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(pi, optimizer):\n",
    "    # Inner gradient-ascent loop of REINFORCE algorithm\n",
    "    T = len(pi.rewards)\n",
    "    rets = np.empty(T, dtype=np.float32)  # the returns\n",
    "    future_ret = 0.0\n",
    "    # compute the returns efficiently\n",
    "    for t in reversed(range(T)):\n",
    "        future_ret = pi.rewards[t] + gamma * future_ret\n",
    "        rets[t] = future_ret\n",
    "\n",
    "    # Compute returns\n",
    "    rets = torch.tensor(rets)\n",
    "    log_probs = torch.stack(pi.log_probs)\n",
    "    \n",
    "    # Compute loss (REINFORCE gradient term)\n",
    "    loss = -log_probs * rets  # Negative for maximizing\n",
    "    loss = torch.sum(loss)\n",
    "\n",
    "    # Perform gradient ascent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()  # Backpropagate, compute gradients\n",
    "    optimizer.step()  # Gradient-ascent, update the weights\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "in_dim = env.observation_space.shape[0]  # 4\n",
    "out_dim = env.action_space.n  # 2\n",
    "\n",
    "pi = Pi(in_dim, out_dim)  # Policy π_θ for REINFORCE\n",
    "optimizer = optim.Adam(pi.parameters(), lr=0.01)\n",
    "\n",
    "for epi in range(300):\n",
    "    state = env.reset()[0]\n",
    "    for t in range(200):  # CartPole max timestep is 200\n",
    "        action = pi.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        pi.rewards.append(reward)\n",
    "        # env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Train the policy per episode\n",
    "    loss = train(pi, optimizer)\n",
    "    total_reward = sum(pi.rewards)\n",
    "    solved = total_reward > 195.0\n",
    "\n",
    "    # On-policy: clear memory after training\n",
    "    pi.onpolicy_reset()\n",
    "    print(f'Episode {epi}, loss: {loss}, '\n",
    "            f'total_reward: {total_reward}, solved: {solved}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "from typing import Optional\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    pi: Pi,\n",
    "    env: gym.Env,\n",
    "    n_eval_episodes: int = 10,\n",
    "    video_name: Optional[str] = None,\n",
    ") -> None:\n",
    "    episode_returns, episode_reward = [], 0.0\n",
    "    total_episodes = 0\n",
    "    done = False\n",
    "\n",
    "    # Setup video recorder\n",
    "    video_recorder = None\n",
    "    if video_name is not None and env.render_mode == \"rgb_array\":\n",
    "        os.makedirs(\"../logs/videos/\", exist_ok=True)\n",
    "\n",
    "        # New gym recorder always wants to cut video into episodes,\n",
    "        # set video length big enough but not to inf (will cut into episodes)\n",
    "        env = RecordVideo(env, \"../logs/videos\", step_trigger=lambda _: False, video_length=100_000)\n",
    "        env.start_recording(video_name)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    n_actions = int(env.action_space.n)\n",
    "\n",
    "    while total_episodes < n_eval_episodes:\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        action = pi.act(obs)\n",
    "\n",
    "\n",
    "        # Send the action to the env\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        ### END OF YOUR CODE\n",
    "\n",
    "        episode_reward += float(reward)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            episode_returns.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "            total_episodes += 1\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    if isinstance(env, RecordVideo):\n",
    "        print(f\"Saving video to ../logs/videos/{video_name}\")\n",
    "        env.close()\n",
    "\n",
    "    print(f\"Total reward = {np.mean(episode_returns):.2f} +/- {np.std(episode_returns):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "eval_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "video_name = f\"PG_{env_id}.mp4\"\n",
    "n_eval_episodes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(pi, eval_env, n_eval_episodes, video_name=video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_tutorial.notebook_utils import show_videos\n",
    "\n",
    "print(f\"PG agent on {env_id} after 300 iterations:\")\n",
    "show_videos(\"../logs/videos/\", prefix=video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Further\n",
    "\n",
    "- Improve the policy gradient algorithm via the natural gradient correction (cf. lecture slides)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
